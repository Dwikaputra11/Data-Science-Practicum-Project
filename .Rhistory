inspect(corpus[1:10])
#fungsinya untuk membersihkan data data yang tidak dibutuhkan
corpus.clean<-corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, c("can","with","was","its","you","but","get","am","is","are","not","the","that", "and", "from", "this", "us", "for", "which","too", "but","there","also","have","only", "were","has","than","very","they","still","will","when","to","your","dont","just","need"))%>%
tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
df.train<-data.frame[1:50,]
df.test<-data.frame[51:100,]
dtm.train<-dtm[1:50,]
dtm.test<-dtm[51:100,]
corpus.clean.train<-corpus.clean[1:50]
corpus.clean.test<-corpus.clean[51:100]
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
#dim(dtm.train.nb)
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
convert_count <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,1,convert_count)
wordcloud(corpus.clean,min.freq = 4,max.words=100,random.order=F,colors=brewer.pal(8,"Dark2"))
kalimat2<-read.csv("Review_GWK_Clean.csv",header=T)
#skoring
kata.positif <- scan("positive-words.txt",what="character",comment.char=";")
kata.negatif <- scan("negative-words.txt",what="character",comment.char=";")
score.sentiment = function(kalimat2, positif, negatif,
.progress='none')
{
require(plyr)
require(stringr)
scores = laply(kalimat2, function(kalimat, positif,
negatif) {
kalimat = gsub('[[:punct:]]', '', kalimat)
kalimat = gsub('[[:cntrl:]]', '', kalimat)
kalimat = gsub('\\d+', '', kalimat)
kalimat = tolower(kalimat)
list.kata = str_split(kalimat, '\\s+')
kata2 = unlist(list.kata)
positif.matches = match(kata2, kata.positif)
negatif.matches = match(kata2, kata.negatif)
positif.matches = !is.na(positif.matches)
negatif.matches = !is.na(negatif.matches)
score = sum(positif.matches) - (sum(negatif.matches))
return(score)
}, kata.positif, kata.negatif, .progress=.progress )
scores.df = data.frame(score=scores, text=kalimat2)
return(scores.df)}
hasil = score.sentiment(kalimat2$text, kata.positif, kata.negatif)
#mengubah nilai score menjadi sentimen
hasil$klasifikasi<- ifelse(hasil$score<0, "Negatif",ifelse(hasil$score==0,"Netral","Positif"))
hasil$klasifikasi
#menukar urutan baris
data <- hasil[c(3,1,2)]
#View(data)
write.csv(data, file = "viewdata.csv")
knitr::opts_chunk$set(echo = TRUE)
require(corpus)
data.frame<-read.csv("Review_GWK_Clean.csv",stringsAsFactors = F)
glimpse(data.frame)
library(tm)
library(wordcloud2)
library(vroom)
library(here)
library(RTextTools)
library(dplyr)
library(wordcloud)
library(shiny)
library(ggplot2)
library(plotly)
library(e1071)
library(caret)
library(syuzhet)
library(here)
library(readxl)
library(vroom)
library(tm)
require(corpus)
data.frame<-read.csv("Review_GWK_Clean.csv",stringsAsFactors = F)
glimpse(data.frame)
set.seed(20)
data.frame<-data.frame[sample(nrow(data.frame)),]
data.frame<-data.frame[sample(nrow(data.frame)),]
glimpse(data.frame)
corpus<-Corpus(VectorSource(data.frame$text))
corpus
inspect(corpus[1:10])
#fungsinya untuk membersihkan data data yang tidak dibutuhkan
corpus.clean<-corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, c("can","with","was","its","you","but","get","am","is","are","not","the","that", "and", "from", "this", "us", "for", "which","too", "but","there","also","have","only", "were","has","than","very","they","still","will","when","to","your","dont","just","need"))%>%
tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
df.train<-data.frame[1:50,]
df.test<-data.frame[51:100,]
dtm.train<-dtm[1:50,]
dtm.test<-dtm[51:100,]
corpus.clean.train<-corpus.clean[1:50]
corpus.clean.test<-corpus.clean[51:100]
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dim(dtm.train.nb)
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
#Boolan Naive Bayes
convert_count <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
#Naive Bayes Model
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,1,convert_count)
#Training
classifier <- naiveBayes(trainNB, df.train$klasifikasi, laplace = 1)
require(corpus)
data.frame<-read.csv("Review_GWK_Clean.csv",stringsAsFactors = F)
glimpse(data.frame)
set.seed(20)
data.frame<-data.frame[sample(nrow(data.frame)),]
data.frame<-data.frame[sample(nrow(data.frame)),]
glimpse(data.frame)
corpus<-Corpus(VectorSource(data.frame$text))
corpus
inspect(corpus[1:10])
#fungsinya untuk membersihkan data data yang tidak dibutuhkan
corpus.clean<-corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, c("can","with","was","its","you","but","get","am","is","are","not","the","that", "and", "from", "this", "us", "for", "which","too", "but","there","also","have","only", "were","has","than","very","they","still","will","when","to","your","dont","just","need"))%>%
tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
df.train<-data.frame[1:50,]
df.test<-data.frame[51:100,]
dtm.train<-dtm[1:50,]
dtm.test<-dtm[51:100,]
corpus.clean.train<-corpus.clean[1:50]
corpus.clean.test<-corpus.clean[51:100]
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dim(dtm.train.nb)
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
#Boolan Naive Bayes
convert_count <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
#Naive Bayes Model
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,2,convert_count)
#Training
classifier <- naiveBayes(trainNB, df.train$klasifikasi, laplace = 1)
require(corpus)
data.frame<-read.csv("Review_GWK_Clean.csv",stringsAsFactors = F)
glimpse(data.frame)
set.seed(20)
data.frame<-data.frame[sample(nrow(data.frame)),]
data.frame<-data.frame[sample(nrow(data.frame)),]
glimpse(data.frame)
corpus<-Corpus(VectorSource(data.frame$text))
corpus
inspect(corpus[1:10])
#fungsinya untuk membersihkan data data yang tidak dibutuhkan
corpus.clean<-corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, c("can","with","was","its","you","but","get","am","is","are","not","the","that", "and", "from", "this", "us", "for", "which","too", "but","there","also","have","only", "were","has","than","very","they","still","will","when","to","your","dont","just","need"))%>%
tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
df.train<-data.frame[1:340,]
df.test<-data.frame[341:680,]
dtm.train<-dtm[1:340,]
dtm.test<-dtm[341:680,]
corpus.clean.train<-corpus.clean[1:340]
corpus.clean.test<-corpus.clean[341:680]
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dim(dtm.train.nb)
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
#Boolan Naive Bayes
convert_count <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
#Naive Bayes Model
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,2,convert_count)
#Training
classifier <- naiveBayes(trainNB, df.train$klasifikasi, laplace = 1)
View(trainNB)
require(corpus)
data.frame<-read.csv("Review_GWK_Clean.csv",stringsAsFactors = F)
glimpse(data.frame)
set.seed(20)
data.frame<-data.frame[sample(nrow(data.frame)),]
data.frame<-data.frame[sample(nrow(data.frame)),]
glimpse(data.frame)
corpus<-Corpus(VectorSource(data.frame$text))
corpus
inspect(corpus[1:10])
#fungsinya untuk membersihkan data data yang tidak dibutuhkan
corpus.clean<-corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, c("can","with","was","its","you","but","get","am","is","are","not","the","that", "and", "from", "this", "us", "for", "which","too", "but","there","also","have","only", "were","has","than","very","they","still","will","when","to","your","dont","just","need"))%>%
tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
df.train<-data.frame[1:340,]
df.test<-data.frame[341:680,]
dtm.train<-dtm[1:340,]
dtm.test<-dtm[341:680,]
corpus.clean.train<-corpus.clean[1:340]
corpus.clean.test<-corpus.clean[341:680]
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dim(dtm.train.nb)
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
#Boolan Naive Bayes
convert_count <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
#Naive Bayes Model
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,2,convert_count)
#Training
classifier <- naiveBayes(trainNB, df.train$klasifikasi, laplace = 2)
require(corpus)
data.frame<-read.csv("Review_GWK_Clean.csv",stringsAsFactors = F)
glimpse(data.frame)
set.seed(20)
data.frame<-data.frame[sample(nrow(data.frame)),]
data.frame<-data.frame[sample(nrow(data.frame)),]
glimpse(data.frame)
corpus<-Corpus(VectorSource(data.frame$text))
corpus
inspect(corpus[1:10])
#fungsinya untuk membersihkan data data yang tidak dibutuhkan
corpus.clean<-corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, c("can","with","was","its","you","but","get","am","is","are","not","the","that", "and", "from", "this", "us", "for", "which","too", "but","there","also","have","only", "were","has","than","very","they","still","will","when","to","your","dont","just","need"))%>%
tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
df.train<-data.frame[1:340,]
df.test<-data.frame[341:680,]
dtm.train<-dtm[1:340,]
dtm.test<-dtm[341:680,]
corpus.clean.train<-corpus.clean[1:340]
corpus.clean.test<-corpus.clean[341:680]
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dim(dtm.train.nb)
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
#Boolan Naive Bayes
convert_count <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
#Naive Bayes Model
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,2,convert_count)
#Training
classifier <- naiveBayes(trainNB, df.train$text, laplace = 2)
#Use the NB classifier we built to make predictions on the test set
pred <- predict(classifier, testNB)
#Create a truth table by tabulating the predicted class labels with the actual predicted class labels with the actual class labels
NB_table=table("Prediction"= pred, "Actual" = df.test$klasifikasi)
#Create a truth table by tabulating the predicted class labels with the actual predicted class labels with the actual class labels
NB_table=table("Prediction"= pred, "Actual" = df.test$)
require(corpus)
data.frame<-read.csv("Review_GWK_Clean.csv",stringsAsFactors = F)
glimpse(data.frame)
set.seed(20)
data.frame<-data.frame[sample(nrow(data.frame)),]
data.frame<-data.frame[sample(nrow(data.frame)),]
glimpse(data.frame)
corpus<-Corpus(VectorSource(data.frame$text))
corpus
inspect(corpus[1:10])
#fungsinya untuk membersihkan data data yang tidak dibutuhkan
corpus.clean<-corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, c("can","with","was","its","you","but","get","am","is","are","not","the","that", "and", "from", "this", "us", "for", "which","too", "but","there","also","have","only", "were","has","than","very","they","still","will","when","to","your","dont","just","need"))%>%
tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
df.train<-data.frame[1:340,]
df.test<-data.frame[341:680,]
dtm.train<-dtm[1:340,]
dtm.test<-dtm[341:680,]
corpus.clean.train<-corpus.clean[1:340]
corpus.clean.test<-corpus.clean[341:680]
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dim(dtm.train.nb)
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
#Boolan Naive Bayes
convert_count <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
#Naive Bayes Model
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,2,convert_count)
#Training
classifier <- naiveBayes(trainNB, df.train$text, laplace = 2)
#Use the NB classifier we built to make predictions on the test set
pred <- predict(classifier, testNB)
#Create a truth table by tabulating the predicted class labels with the actual predicted class labels with the actual class labels
NB_table=table("Prediction"= pred, "Actual" = df.test$text)
NB_table
#confussion Matrix
conf.matNB <- confusionMatrix(pred, df.test$text)
View(classifier)
View(data.frame)
data.frame<-read.csv("labeling.csv",stringsAsFactors = F)
require(corpus)
data.frame<-read.csv("labeling.csv",stringsAsFactors = F)
data.frame$klasifikasi <- factor(data.frame$klasifikasi)
glimpse(data.frame)
set.seed(20)
data.frame<-data.frame[sample(nrow(data.frame)),]
data.frame<-data.frame[sample(nrow(data.frame)),]
glimpse(data.frame)
corpus<-Corpus(VectorSource(data.frame$text))
corpus
inspect(corpus[1:10])
#fungsinya untuk membersihkan data data yang tidak dibutuhkan
corpus.clean<-corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, c("can","with","was","its","you","but","get","am","is","are","not","the","that", "and", "from", "this", "us", "for", "which","too", "but","there","also","have","only", "were","has","than","very","they","still","will","when","to","your","dont","just","need"))%>%
tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
df.train<-data.frame[1:340,]
df.test<-data.frame[341:680,]
dtm.train<-dtm[1:340,]
dtm.test<-dtm[341:680,]
corpus.clean.train<-corpus.clean[1:340]
corpus.clean.test<-corpus.clean[341:680]
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dim(dtm.train.nb)
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
#Boolan Naive Bayes
convert_count <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
#Naive Bayes Model
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,2,convert_count)
#Training
classifier <- naiveBayes(trainNB, df.train$klasifiksi, laplace = 2)
require(corpus)
data.frame<-read.csv("labeling.csv",stringsAsFactors = F)
data.frame$klasifikasi <- factor(data.frame$klasifikasi)
glimpse(data.frame)
set.seed(20)
data.frame<-data.frame[sample(nrow(data.frame)),]
data.frame<-data.frame[sample(nrow(data.frame)),]
glimpse(data.frame)
corpus<-Corpus(VectorSource(data.frame$text))
corpus
inspect(corpus[1:10])
#fungsinya untuk membersihkan data data yang tidak dibutuhkan
corpus.clean<-corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, c("can","with","was","its","you","but","get","am","is","are","not","the","that", "and", "from", "this", "us", "for", "which","too", "but","there","also","have","only", "were","has","than","very","they","still","will","when","to","your","dont","just","need"))%>%
tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
df.train<-data.frame[1:340,]
df.test<-data.frame[341:680,]
dtm.train<-dtm[1:340,]
dtm.test<-dtm[341:680,]
corpus.clean.train<-corpus.clean[1:340]
corpus.clean.test<-corpus.clean[341:680]
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dim(dtm.train.nb)
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
#Boolan Naive Bayes
convert_count <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
#Naive Bayes Model
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,2,convert_count)
#Training
classifier <- naiveBayes(trainNB, df.train$klasifikasi, laplace = 2)
#Use the NB classifier we built to make predictions on the test set
pred <- predict(classifier, testNB)
#Create a truth table by tabulating the predicted class labels with the actual predicted class labels with the actual class labels
NB_table=table("Prediction"= pred, "Actual" = df.test$klasifikasi)
NB_table
#confussion Matrix
conf.matNB <- confusionMatrix(pred, df.test$klasifikasi)
conf.matNB
library(syuzhet) #untuk membaca fungsi get_nrc
dataLabel<- read.csv("viewdata.csv")
ui <- fluidPage(
titlePanel("Analisis sentimen GWK"),
mainPanel(
tabsetPanel(type = "tabs",
tabPanel("Bagan", plotOutput("scatterplot")),
# Plot
tabPanel("Data", DT::dataTableOutput('tbl1')),
# Output Data Dalam Tabel
tabPanel("Wordcloud", plotOutput("Wordcloud"))
)
)
)
# SERVER
server <- function(input, output) {
# Output Data
output$tbl1 = DT::renderDataTable({
DT::datatable(dataLabel, options = list(lengthChange = FALSE))
})
output$scatterplot <- renderPlot({produk_dataset<-read.csv("Review_GWK_Clean.csv",stringsAsFactors = FALSE)
review <-as.character(produk_dataset$text)
s<-get_nrc_sentiment(review)
review_combine<-cbind(produk_dataset$text,s)
par(mar=rep(3,4))
barplot(colSums(s),col= rainbow(10),ylab='count',main='Analisis sentimen GWK')
}, height=400)
output$Wordcloud <- renderPlot({
set.seed(20)
data.frame<-data.frame[sample(nrow(data.frame)),]
data.frame<-data.frame[sample(nrow(data.frame)),]
glimpse(data.frame)
inspect(dtm[1:10,1:20])
df.train<-data.frame[1:50,]
df.test<-data.frame[51:100,]
dtm.train<-dtm[1:50,]
dtm.test<-dtm[51:100,]
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
#dim(dtm.train.nb)
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
convert_count <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,1,convert_count)
wordcloud(corpus.clean,min.freq = 4,max.words=100,random.order=F,colors=brewer.pal(8,"Dark2"))
})
}
shinyApp(ui = ui, server = server)
